{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow\n",
    "\n",
    "\n",
    "Lesson Goals\n",
    "\n",
    "In this lesson you will learn:\n",
    "\n",
    "    The procedure to apply Machine Learning.\n",
    "    The inputs and outputs of every stage of the procedure.\n",
    "    The role of experiments in Machine Learning.\n",
    "\n",
    "Introduction\n",
    "\n",
    "At this moment you may be wondering how to apply Machine Learning to a problem. This lesson provides the answers by breaking down the process into stages and explaining each stage, including the expected inputs and outputs. The Machine Learning workflow is composed of a series of subprocesses, or stages, so that the output of one subprocess is fed as input to the next subprocess. A helpful visual image is a pipeline connecting multiple processing machines.\n",
    "\n",
    "Let's consider as a running example for this lesson an application to eHealth: a project for mining eHRs (electronic Health Records) to improve the quality and effectiveness of health care. More concretely, let's consider the problem of predicting the outcome of a medical treatment prescribed to a patient. Every patient is represented by an instance composed of a set of attributes describing the patient, for example: age, gender, height, weight, risk factors, medication, daily dosage, amount of intakes per day, dosage per intake, lowest blood pressure value, highest blood presure value, etc.\n",
    "\n",
    "You can think of a patient as a vector of values, which are the independent variables. We use Machine Learning to learn a model to estimate the value of the dependent variable \"efficacy of treatment\" that we can model as a real variable in the range [0, 1] with 0 meaning that the treatment is totally ineffective, and 1 meaning totally effective. To evaluate the estimation produced by Machine Learning, we can compare the estimated value to the actual value, and consider the estimation as accurate enough if the difference is 3% at most.\n",
    "\n",
    "In this lesson we will demonstrate the different stages of a machine learning workflow using the census dataset used in previous lessons.\n",
    "ETL\n",
    "\n",
    "ETL stands for Extraction, Transformation and Loading of data. And it is a preliminary process that has to be completed before proceeding to the application of Machine Learning.\n",
    "\n",
    "In the Extraction phase, data is imported from data sources to a data storage with a single unified view, like a Data Warehouse. The data sources may be heterogeneous and distributed. For instance, in our eHealth example, the medical history of a patient may be distributed across different databases belonging to the same or different health systems.\n",
    "\n",
    "In the Transformation phase, the extracted data is homogenized. This includes transforming the data in the following ways:\n",
    "\n",
    "   **Conversion:** magnitudes expressed in different units are all converted to a single reference unit. For instance, all volume magnitudes are converted to milliliters (ml).\n",
    "\n",
    "   **Remove outliers:** an outlier is a data point that clearly does not belong to the distribution of the rest of the dataset. In our eHealth example we can consider that an age of 150 years is certainly an outlier not belonging to the distribution of a representative sample of the general population. Whatever Machine Learning learns from the analysis of this instance is highly likely to not be applicable to the rest of the population. So the outlier is removed. There is another reason to remove the outlier from the data quality perspective. As we will see in the next bullet point, scaling the range of the age variable so that the highest age is mapped to 1, would make the rest of the mapped values to be inadequately compressed.\n",
    "\n",
    "Recall our census dataset. Let's look at the dataset using the describe function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "census=pd.read_csv('../census.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CensusId</th>\n",
       "      <td>1001</td>\n",
       "      <td>1003</td>\n",
       "      <td>1005</td>\n",
       "      <td>1007</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>County</th>\n",
       "      <td>Autauga</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>Blount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalPop</th>\n",
       "      <td>55221</td>\n",
       "      <td>195121</td>\n",
       "      <td>26932</td>\n",
       "      <td>22604</td>\n",
       "      <td>57710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Men</th>\n",
       "      <td>26745</td>\n",
       "      <td>95314</td>\n",
       "      <td>14497</td>\n",
       "      <td>12073</td>\n",
       "      <td>28512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Women</th>\n",
       "      <td>28476</td>\n",
       "      <td>99807</td>\n",
       "      <td>12435</td>\n",
       "      <td>10531</td>\n",
       "      <td>29198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>2.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>75.8</td>\n",
       "      <td>83.1</td>\n",
       "      <td>46.2</td>\n",
       "      <td>74.5</td>\n",
       "      <td>87.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>18.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>46.7</td>\n",
       "      <td>21.4</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Native</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pacific</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Citizen</th>\n",
       "      <td>40725</td>\n",
       "      <td>147695</td>\n",
       "      <td>20714</td>\n",
       "      <td>17495</td>\n",
       "      <td>42345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income</th>\n",
       "      <td>51281</td>\n",
       "      <td>50254</td>\n",
       "      <td>32964</td>\n",
       "      <td>38678</td>\n",
       "      <td>45813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IncomeErr</th>\n",
       "      <td>2391</td>\n",
       "      <td>1263</td>\n",
       "      <td>2973</td>\n",
       "      <td>3995</td>\n",
       "      <td>3141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IncomePerCap</th>\n",
       "      <td>24974</td>\n",
       "      <td>27317</td>\n",
       "      <td>16824</td>\n",
       "      <td>18431</td>\n",
       "      <td>20532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IncomePerCapErr</th>\n",
       "      <td>1080</td>\n",
       "      <td>711</td>\n",
       "      <td>798</td>\n",
       "      <td>1618</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poverty</th>\n",
       "      <td>12.9</td>\n",
       "      <td>13.4</td>\n",
       "      <td>26.7</td>\n",
       "      <td>16.8</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChildPoverty</th>\n",
       "      <td>18.6</td>\n",
       "      <td>19.2</td>\n",
       "      <td>45.3</td>\n",
       "      <td>27.9</td>\n",
       "      <td>27.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Professional</th>\n",
       "      <td>33.2</td>\n",
       "      <td>33.1</td>\n",
       "      <td>26.8</td>\n",
       "      <td>21.5</td>\n",
       "      <td>28.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service</th>\n",
       "      <td>17</td>\n",
       "      <td>17.7</td>\n",
       "      <td>16.1</td>\n",
       "      <td>17.9</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Office</th>\n",
       "      <td>24.2</td>\n",
       "      <td>27.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>17.8</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Construction</th>\n",
       "      <td>8.6</td>\n",
       "      <td>10.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>19</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Production</th>\n",
       "      <td>17.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>23.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drive</th>\n",
       "      <td>87.5</td>\n",
       "      <td>84.7</td>\n",
       "      <td>83.8</td>\n",
       "      <td>83.2</td>\n",
       "      <td>84.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carpool</th>\n",
       "      <td>8.8</td>\n",
       "      <td>8.8</td>\n",
       "      <td>10.9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transit</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walk</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OtherTransp</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorkAtHome</th>\n",
       "      <td>1.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeanCommute</th>\n",
       "      <td>26.5</td>\n",
       "      <td>26.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>28.8</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Employed</th>\n",
       "      <td>23986</td>\n",
       "      <td>85953</td>\n",
       "      <td>8597</td>\n",
       "      <td>8294</td>\n",
       "      <td>22189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PrivateWork</th>\n",
       "      <td>73.6</td>\n",
       "      <td>81.5</td>\n",
       "      <td>71.8</td>\n",
       "      <td>76.8</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PublicWork</th>\n",
       "      <td>20.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>20.8</td>\n",
       "      <td>16.1</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SelfEmployed</th>\n",
       "      <td>5.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FamilyWork</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unemployment</th>\n",
       "      <td>7.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>17.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0        1        2        3        4\n",
       "CensusId            1001     1003     1005     1007     1009\n",
       "State            Alabama  Alabama  Alabama  Alabama  Alabama\n",
       "County           Autauga  Baldwin  Barbour     Bibb   Blount\n",
       "TotalPop           55221   195121    26932    22604    57710\n",
       "Men                26745    95314    14497    12073    28512\n",
       "Women              28476    99807    12435    10531    29198\n",
       "Hispanic             2.6      4.5      4.6      2.2      8.6\n",
       "White               75.8     83.1     46.2     74.5     87.9\n",
       "Black               18.5      9.5     46.7     21.4      1.5\n",
       "Native               0.4      0.6      0.2      0.4      0.3\n",
       "Asian                  1      0.7      0.4      0.1      0.1\n",
       "Pacific                0        0        0        0        0\n",
       "Citizen            40725   147695    20714    17495    42345\n",
       "Income             51281    50254    32964    38678    45813\n",
       "IncomeErr           2391     1263     2973     3995     3141\n",
       "IncomePerCap       24974    27317    16824    18431    20532\n",
       "IncomePerCapErr     1080      711      798     1618      708\n",
       "Poverty             12.9     13.4     26.7     16.8     16.7\n",
       "ChildPoverty        18.6     19.2     45.3     27.9     27.2\n",
       "Professional        33.2     33.1     26.8     21.5     28.5\n",
       "Service               17     17.7     16.1     17.9     14.1\n",
       "Office              24.2     27.1     23.1     17.8     23.9\n",
       "Construction         8.6     10.8     10.8       19     13.5\n",
       "Production          17.1     11.2     23.1     23.7     19.9\n",
       "Drive               87.5     84.7     83.8     83.2     84.9\n",
       "Carpool              8.8      8.8     10.9     13.5     11.2\n",
       "Transit              0.1      0.1      0.4      0.5      0.4\n",
       "Walk                 0.5        1      1.8      0.6      0.9\n",
       "OtherTransp          1.3      1.4      1.5      1.5      0.4\n",
       "WorkAtHome           1.8      3.9      1.6      0.7      2.3\n",
       "MeanCommute         26.5     26.4     24.1     28.8     34.9\n",
       "Employed           23986    85953     8597     8294    22189\n",
       "PrivateWork         73.6     81.5     71.8     76.8       82\n",
       "PublicWork          20.9     12.3     20.8     16.1     13.5\n",
       "SelfEmployed         5.5      5.8      7.3      6.7      4.2\n",
       "FamilyWork             0      0.4      0.1      0.4      0.4\n",
       "Unemployment         7.6      7.5     17.6      8.3      7.7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3220 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3220 non-null   int64  \n",
      " 1   State            3220 non-null   object \n",
      " 2   County           3220 non-null   object \n",
      " 3   TotalPop         3220 non-null   int64  \n",
      " 4   Men              3220 non-null   int64  \n",
      " 5   Women            3220 non-null   int64  \n",
      " 6   Hispanic         3220 non-null   float64\n",
      " 7   White            3220 non-null   float64\n",
      " 8   Black            3220 non-null   float64\n",
      " 9   Native           3220 non-null   float64\n",
      " 10  Asian            3220 non-null   float64\n",
      " 11  Pacific          3220 non-null   float64\n",
      " 12  Citizen          3220 non-null   int64  \n",
      " 13  Income           3219 non-null   float64\n",
      " 14  IncomeErr        3219 non-null   float64\n",
      " 15  IncomePerCap     3220 non-null   int64  \n",
      " 16  IncomePerCapErr  3220 non-null   int64  \n",
      " 17  Poverty          3220 non-null   float64\n",
      " 18  ChildPoverty     3219 non-null   float64\n",
      " 19  Professional     3220 non-null   float64\n",
      " 20  Service          3220 non-null   float64\n",
      " 21  Office           3220 non-null   float64\n",
      " 22  Construction     3220 non-null   float64\n",
      " 23  Production       3220 non-null   float64\n",
      " 24  Drive            3220 non-null   float64\n",
      " 25  Carpool          3220 non-null   float64\n",
      " 26  Transit          3220 non-null   float64\n",
      " 27  Walk             3220 non-null   float64\n",
      " 28  OtherTransp      3220 non-null   float64\n",
      " 29  WorkAtHome       3220 non-null   float64\n",
      " 30  MeanCommute      3220 non-null   float64\n",
      " 31  Employed         3220 non-null   int64  \n",
      " 32  PrivateWork      3220 non-null   float64\n",
      " 33  PublicWork       3220 non-null   float64\n",
      " 34  SelfEmployed     3220 non-null   float64\n",
      " 35  FamilyWork       3220 non-null   float64\n",
      " 36  Unemployment     3220 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 930.9+ KB\n"
     ]
    }
   ],
   "source": [
    "census.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3.220000e+03</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "      <td>3220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31393.605280</td>\n",
       "      <td>9.940935e+04</td>\n",
       "      <td>4.889694e+04</td>\n",
       "      <td>5.051241e+04</td>\n",
       "      <td>11.011522</td>\n",
       "      <td>75.428789</td>\n",
       "      <td>8.665497</td>\n",
       "      <td>1.723509</td>\n",
       "      <td>1.229068</td>\n",
       "      <td>0.082733</td>\n",
       "      <td>...</td>\n",
       "      <td>3.323509</td>\n",
       "      <td>1.612733</td>\n",
       "      <td>4.631770</td>\n",
       "      <td>23.278758</td>\n",
       "      <td>4.559352e+04</td>\n",
       "      <td>74.219348</td>\n",
       "      <td>17.560870</td>\n",
       "      <td>7.931801</td>\n",
       "      <td>0.288106</td>\n",
       "      <td>8.094441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16292.078954</td>\n",
       "      <td>3.193055e+05</td>\n",
       "      <td>1.566813e+05</td>\n",
       "      <td>1.626620e+05</td>\n",
       "      <td>19.241380</td>\n",
       "      <td>22.932890</td>\n",
       "      <td>14.279122</td>\n",
       "      <td>7.253115</td>\n",
       "      <td>2.633079</td>\n",
       "      <td>0.734931</td>\n",
       "      <td>...</td>\n",
       "      <td>3.756096</td>\n",
       "      <td>1.670988</td>\n",
       "      <td>3.178772</td>\n",
       "      <td>5.600466</td>\n",
       "      <td>1.496995e+05</td>\n",
       "      <td>7.863188</td>\n",
       "      <td>6.510354</td>\n",
       "      <td>3.914974</td>\n",
       "      <td>0.455137</td>\n",
       "      <td>4.096114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>8.500000e+01</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>6.200000e+01</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19032.500000</td>\n",
       "      <td>1.121800e+04</td>\n",
       "      <td>5.637250e+03</td>\n",
       "      <td>5.572000e+03</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>64.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>4.550750e+03</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30024.000000</td>\n",
       "      <td>2.603500e+04</td>\n",
       "      <td>1.293200e+04</td>\n",
       "      <td>1.305700e+04</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.050800e+04</td>\n",
       "      <td>75.700000</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46105.500000</td>\n",
       "      <td>6.643050e+04</td>\n",
       "      <td>3.299275e+04</td>\n",
       "      <td>3.348750e+04</td>\n",
       "      <td>9.825000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>26.800000</td>\n",
       "      <td>2.863275e+04</td>\n",
       "      <td>79.700000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>9.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72153.000000</td>\n",
       "      <td>1.003839e+07</td>\n",
       "      <td>4.945351e+06</td>\n",
       "      <td>5.093037e+06</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>85.900000</td>\n",
       "      <td>92.100000</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>71.200000</td>\n",
       "      <td>39.100000</td>\n",
       "      <td>37.200000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>4.635465e+06</td>\n",
       "      <td>88.300000</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>36.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CensusId      TotalPop           Men         Women     Hispanic  \\\n",
       "count   3220.000000  3.220000e+03  3.220000e+03  3.220000e+03  3220.000000   \n",
       "mean   31393.605280  9.940935e+04  4.889694e+04  5.051241e+04    11.011522   \n",
       "std    16292.078954  3.193055e+05  1.566813e+05  1.626620e+05    19.241380   \n",
       "min     1001.000000  8.500000e+01  4.200000e+01  4.300000e+01     0.000000   \n",
       "25%    19032.500000  1.121800e+04  5.637250e+03  5.572000e+03     1.900000   \n",
       "50%    30024.000000  2.603500e+04  1.293200e+04  1.305700e+04     3.900000   \n",
       "75%    46105.500000  6.643050e+04  3.299275e+04  3.348750e+04     9.825000   \n",
       "max    72153.000000  1.003839e+07  4.945351e+06  5.093037e+06    99.900000   \n",
       "\n",
       "             White        Black       Native        Asian      Pacific  ...  \\\n",
       "count  3220.000000  3220.000000  3220.000000  3220.000000  3220.000000  ...   \n",
       "mean     75.428789     8.665497     1.723509     1.229068     0.082733  ...   \n",
       "std      22.932890    14.279122     7.253115     2.633079     0.734931  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%      64.100000     0.500000     0.100000     0.200000     0.000000  ...   \n",
       "50%      84.100000     1.900000     0.300000     0.500000     0.000000  ...   \n",
       "75%      93.200000     9.600000     0.600000     1.200000     0.000000  ...   \n",
       "max      99.800000    85.900000    92.100000    41.600000    35.300000  ...   \n",
       "\n",
       "              Walk  OtherTransp   WorkAtHome  MeanCommute      Employed  \\\n",
       "count  3220.000000  3220.000000  3220.000000  3220.000000  3.220000e+03   \n",
       "mean      3.323509     1.612733     4.631770    23.278758  4.559352e+04   \n",
       "std       3.756096     1.670988     3.178772     5.600466  1.496995e+05   \n",
       "min       0.000000     0.000000     0.000000     4.900000  6.200000e+01   \n",
       "25%       1.400000     0.900000     2.700000    19.500000  4.550750e+03   \n",
       "50%       2.400000     1.300000     3.900000    23.000000  1.050800e+04   \n",
       "75%       4.000000     1.900000     5.600000    26.800000  2.863275e+04   \n",
       "max      71.200000    39.100000    37.200000    44.000000  4.635465e+06   \n",
       "\n",
       "       PrivateWork   PublicWork  SelfEmployed   FamilyWork  Unemployment  \n",
       "count  3220.000000  3220.000000   3220.000000  3220.000000   3220.000000  \n",
       "mean     74.219348    17.560870      7.931801     0.288106      8.094441  \n",
       "std       7.863188     6.510354      3.914974     0.455137      4.096114  \n",
       "min      25.000000     5.800000      0.000000     0.000000      0.000000  \n",
       "25%      70.500000    13.100000      5.400000     0.100000      5.500000  \n",
       "50%      75.700000    16.200000      6.900000     0.200000      7.600000  \n",
       "75%      79.700000    20.500000      9.400000     0.300000      9.900000  \n",
       "max      88.300000    66.200000     36.600000     9.800000     36.500000  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CensusId', 'State', 'County', 'TotalPop', 'Men', 'Women', 'Hispanic',\n",
       "       'White', 'Black', 'Native', 'Asian', 'Pacific', 'Citizen', 'Income',\n",
       "       'IncomeErr', 'IncomePerCap', 'IncomePerCapErr', 'Poverty',\n",
       "       'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction',\n",
       "       'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp',\n",
       "       'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
       "       'SelfEmployed', 'FamilyWork', 'Unemployment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3220 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3220 non-null   int64  \n",
      " 1   State            3220 non-null   object \n",
      " 2   County           3220 non-null   object \n",
      " 3   TotalPop         3220 non-null   int64  \n",
      " 4   Men              3220 non-null   int64  \n",
      " 5   Women            3220 non-null   int64  \n",
      " 6   Hispanic         3220 non-null   float64\n",
      " 7   White            3220 non-null   float64\n",
      " 8   Black            3220 non-null   float64\n",
      " 9   Native           3220 non-null   float64\n",
      " 10  Asian            3220 non-null   float64\n",
      " 11  Pacific          3220 non-null   float64\n",
      " 12  Citizen          3220 non-null   int64  \n",
      " 13  Income           3219 non-null   float64\n",
      " 14  IncomeErr        3219 non-null   float64\n",
      " 15  IncomePerCap     3220 non-null   int64  \n",
      " 16  IncomePerCapErr  3220 non-null   int64  \n",
      " 17  Poverty          3220 non-null   float64\n",
      " 18  ChildPoverty     3219 non-null   float64\n",
      " 19  Professional     3220 non-null   float64\n",
      " 20  Service          3220 non-null   float64\n",
      " 21  Office           3220 non-null   float64\n",
      " 22  Construction     3220 non-null   float64\n",
      " 23  Production       3220 non-null   float64\n",
      " 24  Drive            3220 non-null   float64\n",
      " 25  Carpool          3220 non-null   float64\n",
      " 26  Transit          3220 non-null   float64\n",
      " 27  Walk             3220 non-null   float64\n",
      " 28  OtherTransp      3220 non-null   float64\n",
      " 29  WorkAtHome       3220 non-null   float64\n",
      " 30  MeanCommute      3220 non-null   float64\n",
      " 31  Employed         3220 non-null   int64  \n",
      " 32  PrivateWork      3220 non-null   float64\n",
      " 33  PublicWork       3220 non-null   float64\n",
      " 34  SelfEmployed     3220 non-null   float64\n",
      " 35  FamilyWork       3220 non-null   float64\n",
      " 36  Unemployment     3220 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 930.9+ KB\n"
     ]
    }
   ],
   "source": [
    "census.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for outliers in our census data. Even though the census data is thoroughly cleaned before it is released, for the sake of this exercise, let's pick the FamilyWork feature in our dataset. This column show the percent of people in each county that primarily perform unpaid family work. We can plot the boxplot of the feature to see how many points are extreme outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAHBCAYAAAA8SS9SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc+ElEQVR4nO3df5DcdZ3n8dc7M5MZIWJAvMGIGo64u53MnrLOeorZZNqoKLiAu95pKK9k0xr11jnK6CWcfXsuV9dVDAW7eNktMLGj1Bn7buMvOGHx13avzmUXnYhXR9LrmeUQAw64CwaHDclM531/dCckOAmZ6c73887M81GVYvrXfF9/0PWaz+f7+X6+5u4CACCaBakDAAAwHQoKABASBQUACImCAgCEREEBAEKioAAAIXVnebDzzz/fly5dmuUhgZCefvppnX322aljAMnt2rXrH9z9JdO9lmlBLV26VGNjY1keEgipVqtpaGgodQwgOTP7yYleY4oPABASBQUACImCAgCEREEBAEJ63oIys21m9riZPXDMc+eZ2TfN7Met/557emMCAOabUxlBfU7S257z3PWSvu3ur5L07dZjAAA65nkLyt2/I+mJ5zx9laQ7Wj/fIenqzsYCAMx3sz0H1e/uP2v9PC6pv0N5AACQ1IELdd3dzeyEdz00s/WS1ktSf3+/arVau4cEzngTExN8F4DnMduCeszMXuruPzOzl0p6/ERvdPctkrZI0uDgoHP1PMBOEsCpmO0U312S3tf6+X2S7uxMHAAAmk5lmXlF0t9I+nUz22dmBUk3SnqLmf1Y0ptbjwEA6JhTWcW31t1f6u497n6hu5fd/R/dfY27v8rd3+zuz13lB2AalUpFAwMDWrNmjQYGBlSpVFJHAsLKdDdzYD6rVCoqFosql8tqNBrq6upSoVCQJK1duzZxOiAetjoCMlIqlVQul5XP59Xd3a18Pq9yuaxSqZQ6GhASBQVkpF6va+XKlcc9t3LlStXr9USJgNgoKCAjuVxOo6Ojxz03OjqqXC6XKBEQGwUFZKRYLKpQKKharWpqakrValWFQkHFYjF1NCAkFkkAGTmyEGJ4eFj1el25XE6lUokFEsAJmPsJdynquMHBQR8bG8vseEBU7CQBNJnZLncfnO41pvgAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhNRWQZnZR81st5k9YGYVM+vrVDBgLqpUKhoYGNCaNWs0MDCgSqWSOhIQVvdsP2hmL5P07yQtd/cDZvYXkt4j6XMdygbMKZVKRcViUeVyWY1GQ11dXSoUCpKktWvXJk4HxNPuFF+3pBeYWbeksyQ92n4kYG4qlUoql8vK5/Pq7u5WPp9XuVxWqVRKHQ0IadYjKHd/xMxulvSwpAOSvuHu33ju+8xsvaT1ktTf369arTbbQwJntHq9rkajoVqtpomJCdVqNTUaDdXrdb4XwDTameI7V9JVki6S9AtJO8zsve7++WPf5+5bJG2RpMHBQR8aGpp1WOBMlsvl1NXVpaGhIdVqNQ0NDalarSqXy4nvBfCr2pnie7Ok/+fuP3f3SUlflnRpZ2IBc0+xWFShUFC1WtXU1JSq1aoKhYKKxWLqaEBIsx5BqTm193ozO0vNKb41ksY6kgqYg44shBgeHla9Xlcul1OpVGKBBHAC5u6z/7DZDZLeLWlK0v2S3u/uB0/0/sHBQR8bo8OAI1N8wHxnZrvcfXC619oZQcndPynpk+38DgAApsNOEgCAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCshQpVLRwMCA1qxZo4GBAVUqldSRgLC6UwcA5otKpaJisahyuaxGo6Guri4VCgVJ0tq1axOnA+JhBAVkpFQqqVwuK5/Pq7u7W/l8XuVyWaVSKXU0IKS2CsrMFpvZF83s78ysbmZv6FQwYK6p1+vat2/fcVN8+/btU71eTx0NCKndKb5PSbrX3d9lZgslndWBTMCctGTJEm3cuFFf+MIXjk7xXXPNNVqyZEnqaEBIsy4oM3uRpFWSrpUkdz8k6VBnYgFzk5md9DGAZ7UzgrpI0s8lfdbMXi1pl6Tr3P3pY99kZuslrZek/v5+1Wq1Ng4JnLkeffRRbdq0SevWrdPDDz+sV7ziFbr22ms1MjLC9wKYhrn77D5oNijpbyW90d3vM7NPSXrK3f/oRJ8ZHBz0sbGx2SUFznADAwPavHmz8vm8arWahoaGVK1WNTw8rAceeCB1PCAJM9vl7oPTvdbOIol9kva5+32tx1+U9Ftt/D5gTisWiyoUCqpWq5qamlK1WlWhUFCxWEwdDQhp1lN87j5uZj81s1939x9JWiNpT+eiAXPLkWudhoeHVa/XlcvlVCqVuAYKOIFZT/FJkpm9RtJnJC2U9KCkP3D3J0/0fqb4gKYjU3zAfHeyKb62lpm7+w8lTfuLAQBoBztJAABCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACFRUACAkCgoAEBIFBSQoUqlooGBAa1Zs0YDAwOqVCqpIwFhdacOAMwXlUpFxWJR5XJZjUZDXV1dKhQKkqS1a9cmTgfEwwgKyEipVFK5XFY+n1d3d7fy+bzK5bJKpVLqaEBIFBSQkXq9rpUrVx733MqVK1Wv1xMlAmKjoICM5HI5jY6OHvfc6OiocrlcokRAbBQUkJFisahCoaBqtaqpqSlVq1UVCgUVi8XU0YCQWCQBZOTIQojh4WHV63XlcjmVSiUWSAAnYO6e2cEGBwd9bGwss+MBUdVqNQ0NDaWOASRnZrvcfXC615jiAwCEREEBAEKioAAAIVFQAICQKCgAQEgUFAAgJAoKABASBQUACImCAgCE1HZBmVmXmd1vZl/rRCAAAKTOjKCuk8T9AoBTwB11gVPX1maxZnahpCsklSRt6EgiYI7ijrrAzLQ7grpV0kZJh9uPAsxt3FEXmJlZj6DM7B2SHnf3XWY2dJL3rZe0XpL6+/tVq9Vme0jgjFav19VoNFSr1TQxMaFaraZGo6F6vc73AphGO1N8b5R0pZldLqlP0jlm9nl3f++xb3L3LZK2SM3bbXCLAcxXuVxOXV1dGhoaOnq7jWq1qlwux603gGnMeorP3f+Du1/o7kslvUfSXz23nAA8izvqAjPDHXWBjHBHXWBmOlJQ7l6TVOvE7wIAQGIEBWSGZebAzLDVEZARlpkDM0NBARmp1+tauXLlcc+tXLlS9TobsQDToaCAjORyOY2Ojh733OjoqHK5XKJEQGwUFJARlpkDM8MiCSAjLDMHZoYRFAAgJEZQQEZYZg7MDCMoICMsMwdmhoICMsIyc2BmKCggIywzB2aGggIywjJzYGZYJAFkhGXmwMwwggIAhMQICsgIy8yBmWEEBWSEZebAzFBQQEZYZg7MDAUFZIRl5sDMUFBARlhmDswMiySAjLDMHJgZRlBAhnbu3Km9e/fq8OHD2rt3r3bu3Jk6EhAWIyggI8PDw7r99ts1MjKi5cuXa8+ePdq0aZMkafPmzYnTAfEwggIysnXrVo2MjGjDhg3q6+vThg0bNDIyoq1bt6aOBoTECArIyMGDB/WjH/1IfX19OnjwoHp7e/W+971PBw8eTB0NCImCAjLS1dWlrVu36uabbz46xffxj39cXV1dqaMBITHFB2TE3WVmxz1nZnL3RImA2BhBARk5fPiwPvjBD+oTn/jE0Sm+D3zgA/r0pz+dOhoQEiMoICO9vb2amJjQsmXLtGDBAi1btkwTExPq7e1NHQ0IiYICMrJ69Wpt375dq1at0p133qlVq1Zp+/btWr16depoQEhM8QEZeeSRR3T11Vdr27Ztuu2229Tb26urr75aP/7xj1NHA0KioICM1Ot13X///erp6VGtVtPQ0JAmJyfV19eXOhoQElN8QEbYzRyYGQoKyAi7mQMzwxQfkBF2MwdmxrK8SHBwcNDHxsYyOx4Q1ZFzUMB8Z2a73H1wuteY4gMAhERBAQBCoqAAACFRUECGKpWKBgYGtGbNGg0MDKhSqaSOBITFKj4gI5VKRcViUeVyWY1GQ11dXSoUCpLESj5gGoyggIyUSiWVy2Xl83l1d3crn8+rXC6rVCqljgaEREEBGanX69qxY4f6+vqUz+fV19enHTt2qF6vp44GhMQUH5CRxYsXa8uWLbrpppuO3lF348aNWrx4cepoQEgUFJCRp556Suecc44uueQSNRoNXXLJJTrnnHP01FNPpY4GhERBARmZmprSLbfcctxWR7fccovWrVuXOhoQElsdARnp6+vTueeeq/Hx8aPPXXDBBXryySf1zDPPJEwGpMNWR0AAZ599tsbHx7VixQpVKhWtWLFC4+PjOvvss1NHA0Jiig/IyBNPPKGlS5dq7969Wrt2rXp7e7V06VI99NBDqaMBITGCAjK0ceNGLVu2TAsWLNCyZcu0cePG1JGAsBhBARn62Mc+prvvvvvoThJXXHFF6khAWIyggIz09vbqwIEDuvXWWzUxMaFbb71VBw4cUG9vb+poQEiMoICMTE5OamBgQHfddZfuuusuSdLAwID27NmTOBkQEwUFZCSXy+mqq66Sux+9DurIYwC/ioICMpLP5zUyMqKRkZGjWx1t2rRJH/rQh1JHA0KioICMVKtVbdq0Sdu2bTs6gtq0aZO++tWvpo4GhMROEkBGurq69Mwzz6inp0e1Wk1DQ0OanJxUX1+fGo1G6nhAEuwkAQSQy+U0Ojp63HOjo6PK5XKJEgGxUVBARorFogqFgqrVqqamplStVlUoFFQsFlNHA0LiHBSQkSO3dT92N/NSqcTt3oETYAQFZGjnzp3au3evDh8+rL1792rnzp2pIwFhMYICMjI8PKzbb7/9V5aZS9LmzZsTpwPiYQQFZGTr1q0aGRnRhg0b1NfXpw0bNmhkZERbt25NHQ0IadYFZWYvN7Oqme0xs91mdl0ngwFzzcGDB7Vt2zaZmfL5vMxM27Zt08GDB1NHA0JqZwQ1Jelj7r5c0usl/aGZLe9MLGDuMTPt3r1bV155pb7yla/oyiuv1O7du2VmqaMBIc26oNz9Z+7+g9bPv5RUl/SyTgUD5hp3l5lp9erV6uvr0+rVq2Vm7MUHnEBHFkmY2VJJl0i6b5rX1ktaL0n9/f2q1WqdOCRwRrrssst0/fXXa3JyUj09Pbrssst077338r0AptH2VkdmtkjSX0squfuXT/ZetjrCfGZm6unp0eTk5NHnjjxmFIX56rRtdWRmPZK+JGn785UTMN8tWLBAk5OTWrRokW677TYtWrRIk5OTWrCAxbTAdGY9xWfNM7tlSXV3/5PORQLmpsOHD2vhwoWamJjQhz/8YUnSwoULdejQocTJgJja+dPtjZL+jaQ3mdkPW/8u71AuYE4677zzTvoYwLPaWcU36u7m7v/C3V/T+ndPJ8MBc834+LguvfRS7dixQ5deeqnGx8dTRwLCYvIbyNj+/fs1NTWl/fv3p44ChMZefECGFi9erN27dx/dwXzx4sX6xS9+kTYUEBQjKCBDTz/99EkfA3gWBQVkhGXmwMwwxQdk5PDhw+rp6TlumflzL9wF8Cz+dAMAhERBARmanJxUf3+/PvvZz6q/v5/RE3ASFBSQsYsvvliLFi3SxRdfnDoKEBrnoIAM9ff3a+fOndq5c+fRx4899ljiVEBMjKCADD23jCgn4MQoKCCB4eHh1BGA8CgoIIHNmzenjgCER0EBGerq6jrpYwDPoqCADDUajeOWmTcajdSRgLAoKCBjjz32mB599FEWSADPg4ICEigWi6kjAOFRUACAkCgoIIGNGzemjgCER0EBCdx0002pIwDhUVBAhmq1mtxd1WpV7q5arZY6EhAWe/EBGRoaGkodAThjMIICErj88stTRwDCo6CABO65557UEYDwKCgAQEgUFJDAunXrUkcAwqOggAQWLOCrBzwfviVAAp/5zGdSRwDCo6AAACFRUEACq1atSh0BCI+CAhL4zne+kzoCEB4FBQAIiYICEnjXu96VOgIQHgUFJLBixYrUEYDwKCgggRtuuCF1BCA8CgoAEBIFBSRwwQUXpI4AhEdBAQmMj4+njgCER0EBAEKioIAEli9fnjoCEB4FBSSwZMmS1BGA8CgoIIFvfetbqSMA4VFQQMbcXdVqVe6eOgoQWnfqAMB8Y2Zavny59uzZkzoKEBojKCAjx46Yji0nRlLA9BhBAbNkZsl+D6WG+YARFDBL7j7rf6/c9LW2Pg/MBxQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASOwkgXnv1Td8Q/sPTGZ+3KXX353p8V70gh7970++NdNjAu2goDDv7T8wqYduvCLTY9ZqNQ0NDWV6zKwLEWgXU3wAgJAoKABASEzxYd57Ye56/eYd12d/4DuyPdwLc5KU7VQm0A4KCvPeL+s3cg4KCIgpPgBASG2NoMzsbZI+JalL0mfc/caOpAIylmR0cW/2y8yBM8msC8rMuiT9uaS3SNon6ftmdpe77zn5J4FYsp7ek5qFmOK4wJmknSm+10na6+4PuvshSf9d0lWdiQUAmO/aKaiXSfrpMY/3tZ4DAKBtp30Vn5mtl7Rekvr7+1Wr1U73IYFM5PP5tj5vI7P/bLVabevYwJmgnYJ6RNLLj3l8Yeu547j7FklbJGlwcNCzXloLnC7uPuvPplhmDpxp2pni+76kV5nZRWa2UNJ7JN3VmVgAgPlu1iMod58ys49I+rqay8y3ufvujiUDAMxrbZ2Dcvd7JN3ToSwAABzFThIAgJAoKABASBQUACAkCgoAEBIFBQAIiYICAIREQQEAQqKgAAAhUVAAgJAoKABASBQUACAkCgoAEJK1c0+bGR/M7OeSfpLZAYG4zpf0D6lDAAG80t1fMt0LmRYUgCYzG3P3wdQ5gMiY4gMAhERBAQBCoqCANLakDgBExzkoAEBIjKAAACFRUACAkCgoAEBIFBRwhjEzvreYF/gfHThDmNlCSXL3w2b2RjO7IXUm4HSioIAzgJmdL2mDmb2u9dTFkp5MGAk47bpTBwBwSl4q6UJJbzezCUk9av2BaWZd7t5IGQ44HSgoIDgzM3f/P2b2dUmvlfR2NUdQh8xssaQlZjYl6afufiBhVKCjKCggOHd3M3uLpN+X9CVJvyNpUNJLJJmk35I0JekTkv4mVU6g0ygoIDgzy0m6VtLt7v5dM3tIkks6JGmHu19nZt3uPpUwJtBxLJIAgjKzBa2Ve/9W0m9KuvjIdJ+k/yFpkaR3m9kLJXEOCnMOe/EBgZiZSUen9c5y938ys7Mk/UdJfZI+7+4/aL33tZIm3P1H6RIDpw8FBQRkZm+TdJ2kRyXV3f1mM7tFzXNNX3L37yUNCGSAKT4gADO70Mze3fr5tyXdIunPJW2VdK2Z3Shpo6QXSfpXZnZOsrBARlgkASTWmtZ7naTfN7O/VXMq7+vu/rXW66+VtFPSlyV9UtL57v5UqrxAVhhBAYl5c559t6RfqrmE/BlJbzGzRa3XD0r6K0mL3f0xd9+dLCyQIQoKSMTMXmlmd5jZ2a2FDl+WdKOkn0r6C0ljZvZ6M3u7mhfn/jJhXCBzTPEB6bxB0jslPWFm35H0l5L+i5rnna6UdFDS+yUtkbTJ3bkIF/MKq/iAjJlZj7tPmtmL1VwI8QJJ2yX9rqRvqrnn3sPu/nkz65K00N0PtK6B4guLeYMpPiBDrVK6zcxWuvs/Stqg5kjpaUn/VdIfSnqTpA+b2QvcvXFkfz3KCfMNBQVk61xJD0v6opmtU3MHiG2SLnL370v6A0ljau4cMZAsJRAAU3xAxlp3xH2NmkvG62r+oXhQ0hfcvd66xukF7v5YupRAehQUkJHW9U7nSvq2pHe1nl6p5oKId0j6nqTfc/efH/sZpvYwXzHFB5xmrU1fjxTNk5K+KulV7v73kiqSPqDmCr6LJL3s2M9STpjPKCjgNDGzRa1iOiwpJx0tnL2SbjSzc9z9kLs/Ien3JK129x+mSwzEwhQfcBqY2YskXS9pj6SvqLkQokfSpyX9L0kflvSIu283s4XufihZWCAoRlBAh7W2KPqlpCfUXAzxVknvlfQ/JV2q5r56eUlvkyTKCZgeBQV0UGvktEHNDV//TM3pvDdLeou7b3P3/9R6/fuS3mBma5OFBYJjig/ooFZBnaXmdN4l7n6nmX1IzWuavqvmLdoPm1m3pH8t6Tx3/7N0iYG4GEEBHeTu+yU9LmmNpGvM7Ao1zzs9IOn1ktaa2QJ3n5J0saSrzaznyJ10ATyLggI64EjBmNkid29I+ryku9W83ul31SypB9U8B/XS1sf2S7rO3SdZTg78Kqb4gA4xs6sl/Xs1V+7d7e5fNbNr1bwY9y/VvJ3Gy9394WQhgTMIt9sA2nDkAtzWyr13Srqp9dIfm1mvu3/OzHokXSXpu0fKiR0igOdHQQFtaJXTG9UcJT3m7ndKkpn9k5oX4/a4+1Yzu8fdHz/2c4kiA2cMCgqYhWNGTv9SzRsM/lBSzszeIanq7t9srdS72cy+7e6PpMwLnIk4BwXMkpm9Vs17OH3U3b9nZh+T9Btq7hzx1+7+tJm9uHXfJwAzxCo+YPZc0gpJ10iSu9+i5gKJ90p6U2s5OeUEzBIFBZyiY5eSm9kL3f0Hki6TtMrMNkqSu/+ppPsl/aS1SSyAWWKKDzgFx5xzukrSB9U8f7vV3XeY2aCaU333uvt/ThoUmEMYQQEnYWa/ZmaXtcrprZL+SNL7Jf2dpG1mVnD3MUkfVXNXiH/eumMugDbxRQJOwMx+Tc2bCx7Z+eHFkj4i6bclvVrSekk3mdlH3P0+SUPu/iBTe0BnMMUHTMPMlkvaLumPWxu+mqSlkh6SdKekG919p5n9NzX32Psddx9PlReYixhBAdM7T9Krj1x4K+keSe9sXWD795JWm9nb1TwXdQ3lBHQeBQVMw91HJV1hZg+a2Tcl3efuf9J6+T5J/0zSn0qquPv3U+UE5jKm+ICTMLM1ku6VtPC52xOZ2QXuPs6+esDpwQgKOAl3/7aaG73+XzM7X5Jam7/qyLQe5QScHuzFBzwPd7/HzBqSdpvZb7j7k6kzAfMBU3zAKWrdHfdpd6+lzgLMBxQUMEOccwKyQUEBAEJikQQAICQKCgAQEgUFAAiJggIAhERBAQBCoqAAACH9f4SJO15de1gpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "census.boxplot(column=['FamilyWork'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, quite a bit of data is considered extreme outliers in this boxplot. To get rid of these outliers, we typically remove the entire row that contains the outlier. We compute the outer fence which is 3 times the IQR from the first and third quartiles. We then remove the outliers using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3022 entries, 0 to 3219\n",
      "Data columns (total 37 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CensusId         3022 non-null   int64  \n",
      " 1   State            3022 non-null   object \n",
      " 2   County           3022 non-null   object \n",
      " 3   TotalPop         3022 non-null   int64  \n",
      " 4   Men              3022 non-null   int64  \n",
      " 5   Women            3022 non-null   int64  \n",
      " 6   Hispanic         3022 non-null   float64\n",
      " 7   White            3022 non-null   float64\n",
      " 8   Black            3022 non-null   float64\n",
      " 9   Native           3022 non-null   float64\n",
      " 10  Asian            3022 non-null   float64\n",
      " 11  Pacific          3022 non-null   float64\n",
      " 12  Citizen          3022 non-null   int64  \n",
      " 13  Income           3021 non-null   float64\n",
      " 14  IncomeErr        3021 non-null   float64\n",
      " 15  IncomePerCap     3022 non-null   int64  \n",
      " 16  IncomePerCapErr  3022 non-null   int64  \n",
      " 17  Poverty          3022 non-null   float64\n",
      " 18  ChildPoverty     3021 non-null   float64\n",
      " 19  Professional     3022 non-null   float64\n",
      " 20  Service          3022 non-null   float64\n",
      " 21  Office           3022 non-null   float64\n",
      " 22  Construction     3022 non-null   float64\n",
      " 23  Production       3022 non-null   float64\n",
      " 24  Drive            3022 non-null   float64\n",
      " 25  Carpool          3022 non-null   float64\n",
      " 26  Transit          3022 non-null   float64\n",
      " 27  Walk             3022 non-null   float64\n",
      " 28  OtherTransp      3022 non-null   float64\n",
      " 29  WorkAtHome       3022 non-null   float64\n",
      " 30  MeanCommute      3022 non-null   float64\n",
      " 31  Employed         3022 non-null   int64  \n",
      " 32  PrivateWork      3022 non-null   float64\n",
      " 33  PublicWork       3022 non-null   float64\n",
      " 34  SelfEmployed     3022 non-null   float64\n",
      " 35  FamilyWork       3022 non-null   float64\n",
      " 36  Unemployment     3022 non-null   float64\n",
      "dtypes: float64(27), int64(8), object(2)\n",
      "memory usage: 897.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#census=census[pd.notnull(census.Income)]\n",
    "\n",
    "q1 = np.percentile(census.FamilyWork, 25)\n",
    "q3 = np.percentile(census.FamilyWork, 75)\n",
    "iqr = q3 - q1\n",
    "upper_fence = q3 + 3 * iqr\n",
    "lower_fence = q1 - 3 * iqr\n",
    "#census_without_outliers = census[census.FamilyWork < upper_fence & census.FamilyWork > lower_fence]\n",
    "\n",
    "print (len(census[census.FamilyWork < lower_fence]))\n",
    "census_without_outliers=census[census.FamilyWork < upper_fence]\n",
    "census_without_outliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial dataset has 3220 rows and after removing outliers we have only 3022 rows. A total of 198 rows were removed.\n",
    "\n",
    "   **Scaling:** all variables are scaled so that they take values from the same range, typically this range is [0, 1] (also known as Normalization). This is done to avoid bias effects in error metrics. For instance, consider variable A whose range is [0, 1] and variable B whose range is [0, 100]. A 10% error in the measurement of A would cause a \"noise\" of 0.01, but the same percentage of error in variable B would add a \"noise\" of 10. Thus the overall error might be dominated by errors in B due to scaling reasons, but not necessarily because of the right reason (possibly B has more influence in estimating the right value). Note that if we use an algorithm to scale the training data, then we will have to scale input data with the same algorithm before inputting it to the model learned by Machine Learning.\n",
    "\n",
    "One thing we note in our dataset is that many columns in this dataset are expressed as a percent of the county population. These columns contain a number between 0 and 100. It might be easier for our calculations if we used a number between 0 and 1 and divided all these columns by 100.\n",
    "\n",
    "We can do this one column at a time, for example, here we convert the Hispanic column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9\n",
      "0       0.026\n",
      "1       0.045\n",
      "2       0.046\n",
      "3       0.022\n",
      "4       0.086\n",
      "        ...  \n",
      "3215    0.964\n",
      "3216    0.967\n",
      "3217    0.997\n",
      "3218    0.998\n",
      "3219    0.995\n",
      "Name: HispanicRate, Length: 3220, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(census['Hispanic'].max())\n",
    "census['HispanicRate'] = census['Hispanic'] / 100\n",
    "print(census['HispanicRate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not very efficient. It is more efficient to convery all columns at once. First we make a list of the columns to be converted and then convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.185\n",
       "1    0.095\n",
       "2    0.467\n",
       "3    0.214\n",
       "4    0.015\n",
       "Name: BlackRate, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_percent(x):\n",
    "    return(x/100)\n",
    "\n",
    "conversion_list = ['Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'Employed', 'PrivateWork', 'PublicWork']\n",
    "\n",
    "#we create a new list of columns. We prefer doing this since we'd rather keep the old data around just in case.\n",
    "\n",
    "new_column_list = [x+'Rate' for x in conversion_list]\n",
    "census[new_column_list] = census[conversion_list].apply(to_percent)\n",
    "\n",
    "#Let's look at the Black rate to be sure this worked\n",
    "\n",
    "census.BlackRate.head()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Rounding:** all real values are rounded to the same amount of decimal figures. For instance 0.333333333333 becomes 0.3333.\n",
    "\n",
    "Rounding can be done using the round function in Python.\n",
    "\n",
    "We can round one of the rate columns that we generated. For example, we can round the NativeRate column to reduce the decimal places to 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.004\n",
       "1    0.006\n",
       "2    0.002\n",
       "3    0.004\n",
       "4    0.003\n",
       "Name: NativeRateRounded, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census['NativeRateRounded'] = round(census['NativeRate'], 3)\n",
    "census.NativeRateRounded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Standardization:** transform the data so that it has zero mean (mean removal) and unit variance (variance rescaling). For instance, w = (x-mean) / sigma.\n",
    "\n",
    "We can demonstrate this by standardizing the population size in each county. Scikit-learn (which is a library that we will introduce in greater detail in subsequent lessons) has a function that performs this task called MinMaxScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3220.000000\n",
       "mean        0.009895\n",
       "std         0.031809\n",
       "min         0.000000\n",
       "25%         0.001109\n",
       "50%         0.002585\n",
       "75%         0.006609\n",
       "max         1.000000\n",
       "Name: TotalPopScaled, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#This code uses the reshape function to change the input into a two dimensional numpy array \n",
    "#since this is the input that the function takes.\n",
    "census['TotalPopScaled'] = MinMaxScaler().fit_transform(census.TotalPop.values.reshape(-1,1))\n",
    "census.TotalPopScaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Dummy Variables/One Hot Encoding:** In order to use categorical variables in our models, we perform a transformation that is known in statistics as creating dummy variables and as one hot encoding in computer science. The main idea is to add a variable to the dataset for each value of the categorical variable but one. These variables then take a value of either zero or one to indicate whether the original feature was equal to the value in the new dummy variable column. For example, in the census dataset, we can create a dummy variable for the state feature. We will create a variable for every state and territory but one. The reason for this is that since only one of the dummy variables can be 1 for each row, this means that we can exactly predict the value of one column using the sum of all the rest. This will add a perfectly correlated column to our dataset. As we have shown in previous lessons, this scenario should be avoided.\n",
    "\n",
    "Here is the code for creating a dummy variable for the State feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CensusId', 'State', 'County', 'TotalPop', 'Men', 'Women',\n",
       "       'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific',\n",
       "       'Citizen', 'Income', 'IncomeErr', 'IncomePerCap',\n",
       "       'IncomePerCapErr', 'Poverty', 'ChildPoverty', 'Professional',\n",
       "       'Service', 'Office', 'Construction', 'Production', 'Drive',\n",
       "       'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome',\n",
       "       'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork',\n",
       "       'SelfEmployed', 'FamilyWork', 'Unemployment', 'HispanicRate',\n",
       "       'WhiteRate', 'BlackRate', 'NativeRate', 'AsianRate', 'PacificRate',\n",
       "       'PovertyRate', 'ChildPovertyRate', 'ProfessionalRate',\n",
       "       'ServiceRate', 'OfficeRate', 'ConstructionRate', 'ProductionRate',\n",
       "       'DriveRate', 'CarpoolRate', 'TransitRate', 'WalkRate',\n",
       "       'OtherTranspRate', 'WorkAtHomeRate', 'EmployedRate',\n",
       "       'PrivateWorkRate', 'PublicWorkRate', 'NativeRateRounded',\n",
       "       'TotalPopScaled', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = pd.get_dummies(census.State, drop_first=True)\n",
    "census_dummy = pd.concat([census, states], axis=1)\n",
    "census_dummy.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   **Shuffling:** when working with datasets contained in files (for instance ARFF files or CSV files), it makes sense to shuffle the rows. Otherwise other operations that will happen later in this workflow might be biased. In our eHealth example, if we have the dataset ordered by gender, train the model on the first half, and test it with the second, we would be applying the knowledge learned from the female population to the male population, which might bias the estimations.\n",
    "\n",
    "In Pandas, we can shuffle rows using the sample function. We can take a random sample where we select 100% of our original data (sample proportion = 1). This will result in a random ordering of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_census = census.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset has been transformed, the Loading phase is executed to store the resulting dataset into a data store, for instance a database.\n",
    "\n",
    "\n",
    "**Sampling**\n",
    "\n",
    "After we have performed ETL on the raw data, we need to split the transformed dataset. At this point you have the option to apply Machine Learning in different ways (that will be explained in the following sections). The option that you choose to use is called your \"Experimental Design\". Depending on this experimental design you will need to split your transformed dataset in different ways. For the time being, it is enough to know about the simplest experimental design, known as train-test split. The train-test split means that the transformed dataset is divided into two disjoint subsets: one subset that will be used for training Machine Learning (known as the Training Set), and another to test the model learned by Machine Learning (known as the Test Set).\n",
    "\n",
    "Note that the sampling used to select the train set and the test set must be random. If the transformed dataset is contained in a text file, it is very helpful to have this file shuffled during ETL (as mentioned in the preceding section).\n",
    "\n",
    "Sampling is done using the sample function. Using this function, we can determine the number of rows we want in our sample or the proportion of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample using proportion\n",
    "census_prop_sample = census.sample(frac=0.5)\n",
    "#sample using number of rows\n",
    "census_size_sample = census.sample(n=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Set**\n",
    "\n",
    "A Holdout Set is a subset that we obtain from the transformed dataset and that is not available to Machine Learning during the training phase (this phase is explained in the next section). In the case of the train-test split, the subset used to test the Machine Learning model is a holdout set known as the Test Set. And it is very important that this test set not be available to Machine Learning during training.\n",
    "\n",
    "\n",
    "**Training**\n",
    "\n",
    "After the Training Set is available as a result of the completion of the sampling phase, we are ready to put Machine Learning to work. Our first step is to select the Machine Learning algorithm. After selecting the Machine Learning algorithm, we train it on the Training Set.\n",
    "\n",
    "If we want to perform a train test split, there is a specific function to perform this task called train_test_split in Scikit-learn. Using this function, we pick what proportion of the data will be in the training set and the remainder will be in the test set. Typically we like to have the majority of the dataset in our training set. This is because the more data we use for training, the more accurate our model will be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>CarpoolRate</th>\n",
       "      <th>TransitRate</th>\n",
       "      <th>WalkRate</th>\n",
       "      <th>OtherTranspRate</th>\n",
       "      <th>WorkAtHomeRate</th>\n",
       "      <th>EmployedRate</th>\n",
       "      <th>PrivateWorkRate</th>\n",
       "      <th>PublicWorkRate</th>\n",
       "      <th>NativeRateRounded</th>\n",
       "      <th>TotalPopScaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>12027</td>\n",
       "      <td>Florida</td>\n",
       "      <td>DeSoto</td>\n",
       "      <td>34957</td>\n",
       "      <td>19756</td>\n",
       "      <td>15201</td>\n",
       "      <td>30.5</td>\n",
       "      <td>55.2</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>122.79</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>13197</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Marion</td>\n",
       "      <td>8739</td>\n",
       "      <td>4394</td>\n",
       "      <td>4345</td>\n",
       "      <td>7.2</td>\n",
       "      <td>58.6</td>\n",
       "      <td>31.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.002</td>\n",
       "      <td>32.19</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>21177</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Muhlenberg</td>\n",
       "      <td>31309</td>\n",
       "      <td>15881</td>\n",
       "      <td>15428</td>\n",
       "      <td>0.5</td>\n",
       "      <td>92.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>115.34</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>37061</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Duplin</td>\n",
       "      <td>59453</td>\n",
       "      <td>29280</td>\n",
       "      <td>30173</td>\n",
       "      <td>21.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>24.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>246.54</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>17013</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Calhoun</td>\n",
       "      <td>4999</td>\n",
       "      <td>2528</td>\n",
       "      <td>2471</td>\n",
       "      <td>0.3</td>\n",
       "      <td>97.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.052</td>\n",
       "      <td>22.31</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CensusId           State      County  TotalPop    Men  Women  Hispanic  \\\n",
       "332      12027         Florida      DeSoto     34957  19756  15201      30.5   \n",
       "484      13197         Georgia      Marion      8739   4394   4345       7.2   \n",
       "1081     21177        Kentucky  Muhlenberg     31309  15881  15428       0.5   \n",
       "1920     37061  North Carolina      Duplin     59453  29280  30173      21.2   \n",
       "601      17013        Illinois     Calhoun      4999   2528   2471       0.3   \n",
       "\n",
       "      White  Black  Native  ...  CarpoolRate  TransitRate  WalkRate  \\\n",
       "332    55.2   12.9     0.1  ...        0.180        0.062     0.020   \n",
       "484    58.6   31.5     0.0  ...        0.129        0.003     0.009   \n",
       "1081   92.4    5.2     0.3  ...        0.091        0.002     0.009   \n",
       "1920   52.2   24.8     0.3  ...        0.176        0.000     0.015   \n",
       "601    97.8    0.1     0.1  ...        0.133        0.006     0.014   \n",
       "\n",
       "      OtherTranspRate  WorkAtHomeRate  EmployedRate  PrivateWorkRate  \\\n",
       "332             0.026           0.026        122.79            0.805   \n",
       "484             0.033           0.002         32.19            0.588   \n",
       "1081            0.022           0.016        115.34            0.742   \n",
       "1920            0.010           0.025        246.54            0.765   \n",
       "601             0.017           0.052         22.31            0.770   \n",
       "\n",
       "      PublicWorkRate  NativeRateRounded  TotalPopScaled  \n",
       "332            0.147              0.001        0.003474  \n",
       "484            0.304              0.000        0.000862  \n",
       "1081           0.195              0.003        0.003110  \n",
       "1920           0.150              0.003        0.005914  \n",
       "601            0.128              0.001        0.000490  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "census_train, census_test = train_test_split(census, test_size = 0.2)\n",
    "census_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CensusId</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>...</th>\n",
       "      <th>CarpoolRate</th>\n",
       "      <th>TransitRate</th>\n",
       "      <th>WalkRate</th>\n",
       "      <th>OtherTranspRate</th>\n",
       "      <th>WorkAtHomeRate</th>\n",
       "      <th>EmployedRate</th>\n",
       "      <th>PrivateWorkRate</th>\n",
       "      <th>PublicWorkRate</th>\n",
       "      <th>NativeRateRounded</th>\n",
       "      <th>TotalPopScaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>20135</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>Ness</td>\n",
       "      <td>3077</td>\n",
       "      <td>1561</td>\n",
       "      <td>1516</td>\n",
       "      <td>9.2</td>\n",
       "      <td>89.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.070</td>\n",
       "      <td>15.24</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>21219</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>Todd</td>\n",
       "      <td>12524</td>\n",
       "      <td>6255</td>\n",
       "      <td>6269</td>\n",
       "      <td>4.0</td>\n",
       "      <td>85.4</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.035</td>\n",
       "      <td>46.48</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>42107</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Schuylkill</td>\n",
       "      <td>146360</td>\n",
       "      <td>74521</td>\n",
       "      <td>71839</td>\n",
       "      <td>3.4</td>\n",
       "      <td>92.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.027</td>\n",
       "      <td>632.64</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.014572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>37017</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Bladen</td>\n",
       "      <td>34720</td>\n",
       "      <td>16491</td>\n",
       "      <td>18229</td>\n",
       "      <td>7.6</td>\n",
       "      <td>54.3</td>\n",
       "      <td>34.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.018</td>\n",
       "      <td>130.40</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>51059</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Fairfax</td>\n",
       "      <td>1128722</td>\n",
       "      <td>558606</td>\n",
       "      <td>570116</td>\n",
       "      <td>16.1</td>\n",
       "      <td>52.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.058</td>\n",
       "      <td>6039.66</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.112433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CensusId           State      County  TotalPop     Men   Women  \\\n",
       "955      20135          Kansas        Ness      3077    1561    1516   \n",
       "1102     21219        Kentucky        Todd     12524    6255    6269   \n",
       "2297     42107    Pennsylvania  Schuylkill    146360   74521   71839   \n",
       "1898     37017  North Carolina      Bladen     34720   16491   18229   \n",
       "2848     51059        Virginia     Fairfax   1128722  558606  570116   \n",
       "\n",
       "      Hispanic  White  Black  Native  ...  CarpoolRate  TransitRate  WalkRate  \\\n",
       "955        9.2   89.2    0.5     0.1  ...        0.094        0.001     0.056   \n",
       "1102       4.0   85.4    8.4     0.0  ...        0.107        0.001     0.014   \n",
       "2297       3.4   92.2    2.6     0.1  ...        0.099        0.006     0.036   \n",
       "1898       7.6   54.3   34.7     2.1  ...        0.115        0.004     0.022   \n",
       "2848      16.1   52.6    9.1     0.1  ...        0.094        0.098     0.018   \n",
       "\n",
       "      OtherTranspRate  WorkAtHomeRate  EmployedRate  PrivateWorkRate  \\\n",
       "955             0.011           0.070         15.24            0.602   \n",
       "1102            0.024           0.035         46.48            0.737   \n",
       "2297            0.007           0.027        632.64            0.817   \n",
       "1898            0.005           0.018        130.40            0.720   \n",
       "2848            0.015           0.058       6039.66            0.729   \n",
       "\n",
       "      PublicWorkRate  NativeRateRounded  TotalPopScaled  \n",
       "955            0.221              0.001        0.000298  \n",
       "1102           0.154              0.000        0.001239  \n",
       "2297           0.130              0.001        0.014572  \n",
       "1898           0.234              0.021        0.003450  \n",
       "2848           0.215              0.001        0.112433  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happens During Training?**\n",
    "\n",
    "During training, we apply the machine learning algorithm to our data. The algorithm typically comprised of two mathematical equations. The first equation is the mathematical model. The model describes the mathematical relationship between the features in the data. The second part of the algorithm is the loss function. The loss function quantifies how much information was lost using our model by comparing the actual data with the predicted data that is outputted by the model. Our goal is to optimize the loss. Some machine learning algorithms will continue iterating until achieving an optimal loss.\n",
    "\n",
    "The output of the training phase is a trained model that incorporates the knowledge learned by Machine Learning, which can be queried to solve unseen problems (new problems different from those present in the training set).\n",
    "\n",
    "\n",
    "**Testing**\n",
    "\n",
    "Now, you might be wondering \"how good is the model for solving new problems?\" This is the question that the testing phase answers by computing the value of a quality metric. Usually you will have a threshold value for this quality metric, meaning that for the model to be usefull the value of its quality metric must be equal or greater to the threshold value.\n",
    "\n",
    "It is important to note that the test set cannot be used for training. The fact that Machine Learning is trained with a training set of solved problems (supervised Machine Learning), and the fact that it solves those problems well, does not provide any useful information on how well it generalizes (how well it solves new problems).\n",
    "\n",
    "\n",
    "**Experimental Design**\n",
    "\n",
    "Machine Learning is an experimental science. This means that when you approach a new problem, you don't know beforehand what Machine Learning algorithm will solve the problem satisfactorily. So you have to try different algorithms and analyze what happens. Every time you perform one of these trials you are actually making a Machine Learning experiment. That is why it is said that Machine Learning is an experimental science.\n",
    "\n",
    "Before you perform any Machine Learning experiment, you have to think about how you will proceed. This is what is called the experimental design. Next, you will learn about the most common expèrimental designs that you can try out in solving Machine Learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation**\n",
    "\n",
    "There is a probability that the train-test split tests the model on the only subset where it performs well, thus providing an unreliable quality metric. To reduce this probability, you can test on many different test sets and compute an average of the individual quality metrics obtained. This is what cross validation does. The procedure is:\n",
    "\n",
    "    Randomly partition the dataset in n bins.\n",
    "    For every bin ( b):\n",
    "        Train with the remaining ( n-1) bins.\n",
    "        Test with b and obtain quality metric.\n",
    "    Output the average of the n quality metrics obtained.\n",
    "\n",
    "\n",
    "\n",
    "**Train-Validation-Test Split**\n",
    "\n",
    "The training of some Machine Learning algorithms may be interpreted as learning a set of parameters that minimize the estimation error of the resulting model. How these parameters are learned is controlled by another set of parameters, that are called hyper parameters to avoid the confusion between both sets of parameters. Typical examples of this kind of Machine Learning paradigms are SVMs (Support Vector Machines) and Artificial Neural Networks.\n",
    "\n",
    "The basic procedure is:\n",
    "\n",
    "    Randomly partition the dataset in 3 disjoint subsets (training, validation, and test sets).\n",
    "    Initialize the hyperparameters\n",
    "    Train with the train set.\n",
    "    Evaluate on validation set.\n",
    "    Update best performing hyperparameters.\n",
    "    Recompute/modify the hyperparameters while maximum number of iterations not reached.\n",
    "    Using the best performing hyperparameters, compute the definitive quality metric on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
